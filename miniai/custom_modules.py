# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/#custom_modules.ipynb.

# %% auto 0
__all__ = ['conv', 'GeneralReLU']

# %% ../nbs/#custom_modules.ipynb 1
import torch, torch.nn as nn, torch.nn.functional as F

# %% ../nbs/#custom_modules.ipynb 2
def conv(in_f, out_f, ks=3, stride=2, act=None, norm=None):
    res = nn.Conv2d(in_f, out_f, ks, stride, padding=ks//2)
    if (act is None) and (norm is None): return res
    if (act is None): return nn.Sequential(res, norm(out_f))
    if (norm is None): return nn.Sequential(res, act)
    return nn.Sequential(res, act, norm(out_f))

# %% ../nbs/#custom_modules.ipynb 3
class GeneralReLU(nn.Module):
    def __init__(self, sub=None, leaky=None, maxv=None): 
        super().__init__()
        self.leaky = leaky; self.sub = sub; self.maxv = maxv
    def forward(self, x):
        x = F.leaky_relu(x, self.leaky) if (self.leaky != None) else F.relu(x)
        if self.sub != None: x -= self.sub
        if self.maxv != None: x.clamp_max_(self.maxv)
        return x
