# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/#learner.ipynb.

# %% auto 0
__all__ = ['Learner', 'TrainLearner', 'MomentumLearner']

# %% ../nbs/#learner.ipynb 1
import torch, torch.nn as nn, torch.nn.functional as F
from torch import tensor, optim
import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl, math
import fastcore.all as fc
from operator import attrgetter
from collections.abc import Mapping
from functools import partial
from .callbacks import *

# %% ../nbs/#learner.ipynb 9
class Learner:
    def __init__(self, dls, model, loss_func, opt_func=optim.Adam, lr=None, cbs=[]): 
        fc.store_attr()
        
    @with_cbs('batch')
    def _one_batch(self):
        # Get the gradients by calculating the loss
        self.predict()
        self.get_loss()
        # Update the weights
        if self.training:
            self.backward()
            self.step()
            self.zero_grad()
            
    @with_cbs('epoch')
    def _one_epoch(self):
        for self.batch_iter, self.batch in enumerate(self.dl): self._one_batch()
    
    @with_cbs('fit')
    def _fit(self):
        for self.epoch in self.epochs:
            self.one_epoch(True)
            with torch.no_grad(): self.one_epoch(False)
    
    def one_epoch(self, train):
        self.model.train(train)
        self.training = train
        self.dl = self.dls.train if train else self.dls.valid
        self._one_epoch()

    def fit(self, epochs, lr=None):
        if (lr != None): self.lr = lr
        self.opt = self.opt_func(self.model.parameters(), self.lr)
        self.epochs = range(epochs)
        self._fit()
            
    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)
        
    def __getattr__(self, nm):
        if nm in ['predict', 'get_loss', 'backward', 'step', 'zero_grad']: return partial(self.callback, nm)
        raise AttributeError(nm)

# %% ../nbs/#learner.ipynb 10
class TrainLearner(Learner):
    def predict(self): self.preds = self.model(self.batch[0])
    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])
    def backward(self): self.loss.backward()
    def step(self): self.opt.step()
    def zero_grad(self): self.opt.zero_grad()

# %% ../nbs/#learner.ipynb 11
class MomentumLearner(TrainLearner):
    def __init__(self, dls, model, loss_func, opt=optim.Adam, lr=None, cbs=[], mom=0.85):
        super(TrainLearner, self).__init__(dls, model, loss_func, opt_func=opt, lr=lr, cbs=cbs)
        self.mom = mom

    def zero_grad(self): 
        with torch.no_grad():
            for p in self.model.parameters(): p.grad = p.grad*self.mom
