{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe98206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86843e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch import tensor, optim\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl, math\n",
    "import fastcore.all as fc\n",
    "from operator import attrgetter\n",
    "from collections.abc import Mapping\n",
    "from functools import partial\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from torcheval.metrics import Mean\n",
    "from miniai.plotting import subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e67ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Since there were some problems with torcheval metrics when tensors which are on GPU\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    res = x.detach().cpu()\n",
    "    return res.float() if res.dtype==torch.float16 else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971a213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d207c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelEpochException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "\n",
    "class with_cbs:\n",
    "    def __init__(self, cb_nm): self.cb_nm = cb_nm\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f'before_{self.cb_nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'after_{self.cb_nm}')\n",
    "            except globals()[f'Cancel{self.cb_nm.title()}Exception']: pass\n",
    "        return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8951fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): \n",
    "    order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88959e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, torch.Tensor): return x.to(device)\n",
    "    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n",
    "    return type(x)(to_device(o, device) for o in x)\n",
    "\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): self.device=device\n",
    "    def before_fit(self, learn): \n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab09902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainCB(Callback):\n",
    "    order = 0\n",
    "    def predict(self, learn): learn.preds = learn.model(learn.batch[0])\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, learn.batch[1])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e187f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MetricCB(Callback):\n",
    "    order = TrainCB.order+1\n",
    "    def __init__(self, include_train=False, **kwargs): \n",
    "        self.include_train = include_train; self.metric_list = kwargs; self.loss = Mean()\n",
    "        \n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): \n",
    "        if self.include_train or (not learn.training):\n",
    "            for metric in self.metric_list.values(): metric.reset()\n",
    "        self.loss.reset()\n",
    "    \n",
    "    def after_batch(self, learn): \n",
    "        cpu_preds = to_cpu(learn.preds); cpu_y_b = to_cpu(learn.batch[1])\n",
    "        if self.include_train or (not learn.training):\n",
    "            for metric in self.metric_list.values(): metric.update(cpu_preds, cpu_y_b)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(cpu_y_b))\n",
    "    def after_epoch(self, learn):\n",
    "        if learn.training: \n",
    "            self.log = {}\n",
    "            self.log['epoch'] = learn.epoch\n",
    "            self.log['train_loss'] = round(float(self.loss.compute()), 3)\n",
    "            if self.include_train:\n",
    "                for metric_nm in self.metric_list:\n",
    "                    self.log[f'train_{metric_nm}'] = round(float(self.metric_list[metric_nm].compute()), 3)\n",
    "        else: \n",
    "            prefix = 'valid_' if self.include_train else ''\n",
    "            self.log['valid_loss'] = round(float(self.loss.compute()), 3)\n",
    "            for metric_nm in self.metric_list:\n",
    "                self.log[f'{prefix}{metric_nm}'] = round(float(self.metric_list[metric_nm].compute()), 3)\n",
    "            self._log(self.log)\n",
    "    \n",
    "    def _log(self, d):\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80679bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricCB.order+1\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        learn.metrics._log = self._log\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b9405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, coef=1.3): self.coef = coef; self.lrs = []; self.losses = []\n",
    "    def before_fit(self, learn):\n",
    "        self.lrs = []; self.losses = []; self.min_loss = math.inf\n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training: raise CancelEpochException()\n",
    "        loss = to_cpu(learn.loss)\n",
    "        self.min_loss = min(self.min_loss, loss)\n",
    "        if math.isnan(loss) or (3*self.min_loss < loss): raise CancelFitException()\n",
    "        self.lrs.append(learn.opt.param_groups[0]['lr'])\n",
    "        self.losses.append(loss)\n",
    "        for p in learn.opt.param_groups: p['lr'] *= self.coef\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "867869a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BaseSchedCB(Callback):\n",
    "    def __init__(self, sched): self.sched = sched\n",
    "    def before_fit(self, learn): self.schedo = self.sched(learn.opt)\n",
    "    def step(self, learn):\n",
    "        if learn.training: self.schedo.step()\n",
    "\n",
    "class BatchSchedCB(BaseSchedCB):\n",
    "    def after_batch(self, learn): \n",
    "        self.step(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fb0f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RecorderCB(Callback):\n",
    "    def __init__(self, **d): self.d = d\n",
    "    def before_fit(self, learn):\n",
    "        self.recs = {k:[] for k in self.d}\n",
    "        self.pg = learn.opt.param_groups[0]\n",
    "    def after_batch(self, learn):\n",
    "        for k, v in self.d.items():\n",
    "            self.recs[k].append(v(self.pg))\n",
    "    def plot(self, **kwargs):\n",
    "        fig, ax = subplots(n=len(self.d), **kwargs)\n",
    "        for i, k in enumerate(self.d):\n",
    "            ax[i].plot(self.recs[k], label=k)\n",
    "            ax[i].legend()\n",
    "        plt.tight_layout(); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d67aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c941aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77f19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
